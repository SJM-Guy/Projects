{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sNKZq4XrXQh"
      },
      "source": [
        "# <font color='red'><b>Bootstrap Implementation in Random Forest</b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuxBq_bvrwh2"
      },
      "source": [
        "## Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6ag91ijrQOs"
      },
      "outputs": [],
      "source": [
        "import numpy as np # importing numpy for numerical computation\n",
        "from sklearn.datasets import load_boston # here we are using sklearn's boston dataset\n",
        "from sklearn.metrics import mean_squared_error # importing mean_squared_error metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcHOsONTt1K_"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "boston = load_boston()\n",
        "x=boston.data #independent variables\n",
        "y=boston.target #target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "pc1htEFYuLRj",
        "outputId": "f5b60712-98b3-4cdc-b629-3546c1e3859c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(506, 13)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "kQle3T_wuOa3",
        "outputId": "521c7bdd-5316-48d5-c534-b61d170d2c28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n",
              "        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02,\n",
              "        1.5300e+01, 3.9690e+02, 4.9800e+00],\n",
              "       [2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
              "        6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
              "        1.7800e+01, 3.9690e+02, 9.1400e+00],\n",
              "       [2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
              "        7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
              "        1.7800e+01, 3.9283e+02, 4.0300e+00],\n",
              "       [3.2370e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01,\n",
              "        6.9980e+00, 4.5800e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02,\n",
              "        1.8700e+01, 3.9463e+02, 2.9400e+00],\n",
              "       [6.9050e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01,\n",
              "        7.1470e+00, 5.4200e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02,\n",
              "        1.8700e+01, 3.9690e+02, 5.3300e+00]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# display part of the data\n",
        "x[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEa_HqRZloH4"
      },
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ5q8IxHNRk3"
      },
      "source": [
        "### <font color='red'> <b>Step - 1</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJCFCaOzl7Mr"
      },
      "source": [
        "*  <b>Procedure for Creating samples</b><br>\n",
        "    <b> Randomly create 30 samples from the whole boston data points</b>\n",
        "    *  Creating each sample: Consider any random 303(60% of 506) data points from whole data set and then replicate any 203 points from the sampled points\n",
        "    \n",
        "     For better understanding of this procedure lets check this examples, assume we have 10 data points [1,2,3,4,5,6,7,8,9,10], first we take 6 data points randomly , consider we have selected [4, 5, 7, 8, 9, 3] now we will replicate 4 points from [4, 5, 7, 8, 9, 3], consder they are [5, 8, 3,7] so our final sample will be [4, 5, 7, 8, 9, 3, 5, 8, 3,7]\n",
        "* <b> Create 30 samples </b>\n",
        "    *  Note that as a part of the Bagging when we are taking the random samples <b>we have to make sure each of the sample will have different set of columns</b><br>\n",
        "Ex: Assume we have 10 columns[1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10] for the first sample we will select [3, 4, 5, 9, 1, 2] and for the second sample  [7, 9, 1, 4, 5, 6, 2] and so on...\n",
        "Each sample will have atleast 3 feautres/columns/attributes\n",
        "\n",
        "* <font color='red'><b> Note - While selecting the first random 60% datapoints from the whole data, we are to make sure that the selected datapoints are all exclusive, repetition is not allowed. </b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUqFEBSvNjCa"
      },
      "source": [
        "### <font color='red'><b>Step - 2 </b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqi9AhCYNq3Z"
      },
      "source": [
        "### Building High Variance Models on each of the sample and finding train MSE value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lLBnZHXOFln"
      },
      "source": [
        "*  Build a regression trees on each of 30 samples.\n",
        "*  Computed the predicted values of each data point(506 data points) in your corpus.\n",
        "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{30}\\sum_{k=1}^{30}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$\n",
        "*  Now calculate the $MSE =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kls23JLnSN23"
      },
      "source": [
        "###<font color='red'> <b>Step - 3 </b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz2GchkGSWnh"
      },
      "source": [
        "### Calculating the OOB score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGHkVV2kSibm"
      },
      "source": [
        "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{k}\\sum_{\\text{k= model which was buit on samples not included } x^{i}}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.\n",
        "*  Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK860ocxTyoz"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dme-N6TUCrY"
      },
      "source": [
        "*  Computing CI of OOB Score and Train MSE\n",
        "  *   We will repeat Part 1 for 35 times, and for each iteration store the Train MSE and OOB score.\n",
        "<li> After this we will have 35 Train MSE values and 35 OOB scores </li>\n",
        "<li> using these 35 values we will find the confidence intravels of MSE and OOB Score </li>\n",
        "<li> We will print the CI of MSE and CI of OOB Score </li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6UcH1x9Uwrj"
      },
      "source": [
        "# Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOC_AgsLU7OH"
      },
      "source": [
        "*  Given a single query point predict the price of house."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYs5jSFdVILe"
      },
      "source": [
        "Consider xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] \n",
        "Predict the house price for this point as mentioned in the step 2 of Task 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6rShd89t552"
      },
      "source": [
        "## <font color='red'><b>A few key points</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdgTUXTouHEd"
      },
      "source": [
        "* Remember that the datapoints used for calculating MSE score contain some datapoints that were initially used while training the base learners (the 60% sampling). This makes these datapoints partially seen (i.e. the datapoints used for calculating the MSE score are a mixture of seen and unseen data).\n",
        "Whereas, the datapoints used for calculating OOB score have only the unseen data. This makes these datapoints completely unseen and therefore appropriate for testing the model's performance on unseen data.\n",
        "\n",
        "* Given the information above, if your logic is correct, the calculated MSE score should be less than the OOB score.\n",
        "\n",
        "* The MSE score must lie between 0 and 10.\n",
        "* The OOB score must lie between 10 and 35.\n",
        "\n",
        "* The difference between the left nad right confidence-interval values must not be more than 10. Make sure this is true for both MSE and OOB confidence-interval values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2fHTdS_zpgG"
      },
      "source": [
        "# Implementing Part - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJXX8vf3z073"
      },
      "source": [
        "* Creating samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSVaWG1F4uCZ"
      },
      "source": [
        "<font color='Orange'><b>Algorithm</b></font>\n",
        "\n",
        "![alt text](https://i.imgur.com/OfcFrUP.jpg/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph_6D2SDzz7F"
      },
      "outputs": [],
      "source": [
        "def generating_samples(input_data, target_data):\n",
        "\n",
        "    '''In this function, we will write code for generating 30 samples '''\n",
        "    \n",
        "    # We will follow above pseudo code for generating samples \n",
        "    \n",
        "    selecting_rows, selecting_columns = [], []   # Initialising the lists to store the column and row samples\n",
        "    \n",
        "    # First we need to select the random 60% data points(or indices here) \"without replacement\"\n",
        "    selecting_rows = np.random.choice(np.arange(0, len(input_data)),size=303, replace=False)\n",
        "    \n",
        "    # Selecting rows with replacement from the above list\n",
        "    replacing_rows = np.random.choice(np.arange(0,len(selecting_rows)),size=203, replace=True)\n",
        "    \n",
        "    # Now selecting \"unique\" column indices, i.e. SRSWOR\n",
        "    selecting_columns = np.random.choice(np.arange(0, 13),size=np.random.randint(low=3, high=14), replace=False)\n",
        "    \n",
        "    # selecting the data from our dataset based on the obtained indices\n",
        "    sample_data = input_data[selecting_rows[:,None],selecting_columns]\n",
        "    \n",
        "    target_of_sample_data = target_data[selecting_rows]\n",
        "    \n",
        "    replicated_sample_data = sample_data[replacing_rows]\n",
        "    \n",
        "    target_of_replicated_sample_data = target_of_sample_data[replacing_rows]\n",
        "    \n",
        "    # Stack all the data and return them\n",
        "    final_sample_data = np.vstack((sample_data,replicated_sample_data))\n",
        "    final_target_data = np.vstack((target_of_sample_data.reshape(-1,1),target_of_replicated_sample_data.reshape(-1,1)))\n",
        "    \n",
        "    return list(final_sample_data), list(final_target_data), list(selecting_rows), list(selecting_columns)\n",
        "\n",
        "    # return sampled_input_data , sampled_target_data,selected_rows,selected_columns\n",
        "    #note please return as lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4LSsmn4Jn2_"
      },
      "source": [
        "*  Create 30 samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ec7MN6sL2BZ"
      },
      "source": [
        "![alt text](https://i.imgur.com/p8eZaWL.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXlKWjCcBvTk"
      },
      "outputs": [],
      "source": [
        "# Use generating_samples function to create 30 samples \n",
        "# store these created samples in a list\n",
        "list_input_data =[]\n",
        "list_output_data =[]\n",
        "list_selected_row= []\n",
        "list_selected_columns=[]\n",
        "\n",
        "for i in range(30):\n",
        "    a,b,c,d = generating_samples(x, y)\n",
        "    list_input_data.append(a)\n",
        "    list_output_data.append(b)\n",
        "    list_selected_row.append(c)\n",
        "    list_selected_columns.append(d)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBy4zXSWPtU8"
      },
      "source": [
        "<font color='orange'><b>Flowchart for building tree</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xvH06HPQBdP"
      },
      "source": [
        "![alt text](https://i.imgur.com/pcXfSmp.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWQp6tRwMthq"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# we will use a dictionary to store the all the models\n",
        "models = []\n",
        "for i in range(30):\n",
        "    \n",
        "    # We will fit our decision tree model with the data generated above\n",
        "    # Declaring our decision tree regressor\n",
        "    clf = DecisionTreeRegressor(max_depth=None)\n",
        "    clf.fit(list_input_data[i],list_output_data[i])\n",
        "    \n",
        "    # Storing the models in a list for further use\n",
        "    models.append(clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21j8BKfAQ1U8"
      },
      "source": [
        "<font color='orange'><b>Flowchart for calculating MSE </b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q0mTBD2RBx_"
      },
      "source": [
        "![alt text](https://i.imgur.com/sPEE618.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e-UamlHRjPy"
      },
      "source": [
        "After getting predicted_y for each data point, we can use sklearns mean_squared_error to calculate the MSE between predicted_y and actual_y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWhcvMRWRA9b",
        "outputId": "97ca33c2-0fe1-4703-a4f0-a7d9505b3242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.15115013781193626\n"
          ]
        }
      ],
      "source": [
        "# Creating an empty list to store the predicted values of y\n",
        "predicted_y = []\n",
        "\n",
        "for i in range(len(x)):\n",
        "    \n",
        "    # creating a temporary list to store the outputs of 30 models which will then be aggregated using median\n",
        "    pred_y = []\n",
        "    # Iterating over all the saved models to get prediction for a paricular datapoint\n",
        "    for j in range(30):\n",
        "        # Selecting the columns for the given model\n",
        "        datapoint = x[i,list_selected_columns[j]]\n",
        "        \n",
        "        pred_y.append(models[j].predict(datapoint.reshape(1,-1)))\n",
        "    \n",
        "    # Using the median to aggregate all the outputs\n",
        "    predicted_y.append(np.median(pred_y))\n",
        "\n",
        "# calculating MSE score\n",
        "mse_score = mean_squared_error(y,predicted_y)\n",
        "print(mse_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESb9FSIDTM5V"
      },
      "source": [
        "<font color='orange'><b>Flowchart for calculating OOB score</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB-d6NMETbd9"
      },
      "source": [
        "![alt text](https://i.imgur.com/95S5Mtm.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW3GOcFzTqbt"
      },
      "source": [
        "Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBqcS03pUYSZ"
      },
      "source": [
        "* Code for calculating OOB score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fog_6DNdS-h_",
        "outputId": "4bc8e545-a217-40b7-dfe3-58882d9dab82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13.36445234045715\n"
          ]
        }
      ],
      "source": [
        "# Calculating OOB score\n",
        "\n",
        "# Creating a list to store the predicted values of oob data points\n",
        "predicted_oob_y = []\n",
        "\n",
        "for i in range(len(x)):\n",
        "    \n",
        "    # creating a temporary list to store predicted values from all the models for a particular datapoint\n",
        "    pred_oob_y = []\n",
        "    \n",
        "    for j in range(30):\n",
        "        \n",
        "        # Checking if the index \"i\" is present in the list of selected rows corresponding to the model j\n",
        "        if i not in list_selected_row[j]:\n",
        "            # Extract the columns required for the jth model\n",
        "            datapoint = x[i,list_selected_columns[j]]\n",
        "            \n",
        "            # storing the predicted y for each model\n",
        "            pred_oob_y.append(models[j].predict(datapoint.reshape(1,-1)))\n",
        "    \n",
        "    # appending the median value of the predicted base y's to get the final predicted value for ith datapoint\n",
        "    predicted_oob_y.append(np.median(pred_oob_y))\n",
        "\n",
        "# calculating the OOB score as per the given formula\n",
        "oob_score = mean_squared_error(y, predicted_oob_y)\n",
        "print(oob_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbuiwX3OUjUI"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceW5-D88Uswi",
        "outputId": "a15d6afc-bd11-4533-f493-3fa4c27d0fcd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [02:05<00:00,  3.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CI of MSE is: (0.06273894679611279, 0.10581349757281876)\n",
            "CI of OOB Score is: (12.840101088316112, 13.844243657079774)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# find Confidence Interval of MSE and OOB_score\n",
        "\n",
        "from tqdm import tqdm\n",
        "# Appending all the subsections of task1 in one single cell\n",
        "mse, oob = [], []\n",
        "\n",
        "for i in tqdm(range(35)):\n",
        "    \n",
        "    # ********************** Generating Bootstrap Samples *************************************************\n",
        "    list_input_data_1 = []\n",
        "    list_output_data_1 = []\n",
        "    list_selected_row_1 = []\n",
        "    list_selected_columns_1 = []\n",
        "    \n",
        "    for _ in range(30):\n",
        "        a,b,c,d = generating_samples(x, y)\n",
        "        list_input_data_1.append(a)\n",
        "        list_output_data_1.append(b)\n",
        "        list_selected_row_1.append(c)\n",
        "        list_selected_columns_1.append(d)\n",
        "        \n",
        "    \n",
        "    # ********************** Base Model creation *************************************************\n",
        "    \n",
        "    # we will use a dictionary to store the all the models\n",
        "    base_models = []\n",
        "    for i in range(30):\n",
        "\n",
        "        # We will fit our decision tree model with the data generated above\n",
        "        # Declaring our decision tree regressor\n",
        "        clf = DecisionTreeRegressor(max_depth=None)\n",
        "        clf.fit(list_input_data_1[i],list_output_data_1[i])\n",
        "\n",
        "        # Storing the models in a list for further use\n",
        "        base_models.append(clf)\n",
        "    \n",
        "    # ******************** MSE Calculation ********************************************************\n",
        "    \n",
        "    # Creating an empty list to store the predicted values of y\n",
        "    predicted_y = []\n",
        "\n",
        "    for i in range(len(x)):\n",
        "\n",
        "        # creating a temporary list to store the outputs of 30 models which will then be aggregated using median\n",
        "        pred_y = []\n",
        "        # Iterating over all the saved models to get prediction for a paricular datapoint\n",
        "        for j in range(30):\n",
        "            # Selecting the columns for the given model\n",
        "            datapoint = x[i,list_selected_columns_1[j]]\n",
        "\n",
        "            pred_y.append(base_models[j].predict(datapoint.reshape(1,-1)))\n",
        "\n",
        "        # Using the median to aggregate all the outputs\n",
        "        predicted_y.append(np.median(pred_y))\n",
        "\n",
        "    # calculating MSE score\n",
        "    mse_score = mean_squared_error(y,predicted_y)\n",
        "    mse.append(mse_score)\n",
        "\n",
        "\n",
        "    # ***************************** OOB calcualtion ******************************************\n",
        "    \n",
        "    # Creating a list to store the predicted values of oob data points\n",
        "    predicted_oob_y = []\n",
        "\n",
        "    for i in range(len(x)):\n",
        "\n",
        "        # creating a temporary list to store predicted values from all the models for a particular datapoint\n",
        "        pred_oob_y = []\n",
        "\n",
        "        for j in range(30):\n",
        "\n",
        "            # Checking if the index \"i\" is present in the list of selected rows corresponding to the model j\n",
        "            if i not in list_selected_row_1[j]:\n",
        "                # Extract the columns required for the jth model\n",
        "                datapoint = x[i,list_selected_columns_1[j]]\n",
        "\n",
        "                # storing the predicted y for each model\n",
        "                pred_oob_y.append(base_models[j].predict(datapoint.reshape(1,-1)))\n",
        "\n",
        "        # appending the median value of the predicted base y's to get the final predicted value for ith datapoint\n",
        "        predicted_oob_y.append(np.median(pred_oob_y))\n",
        "\n",
        "    # calculating the OOB score as per the given formula\n",
        "    oob_score = mean_squared_error(y, predicted_oob_y)\n",
        "    oob.append(oob_score)\n",
        "\n",
        "    \n",
        "# using the samples to calculate CI for MSE and OOB score with the below formula\n",
        "# ci_of_x = [mean(x)-1.96*SE(x),mean(x)+1.96*SE(x)]\n",
        "\n",
        "ci_of_mse = [np.mean(mse)-1.96*(np.std(mse)/np.sqrt(len(mse))),np.mean(mse)+1.96*(np.std(mse)/np.sqrt(len(mse)))]\n",
        "\n",
        "ci_of_oob = [np.mean(oob)-1.96*(np.std(oob)/np.sqrt(len(oob))),np.mean(oob)+1.96*(np.std(oob)/np.sqrt(len(oob)))]\n",
        "\n",
        "print(f\"CI of MSE is: ({ci_of_mse[0]}, {ci_of_mse[1]})\")\n",
        "print(f\"CI of OOB Score is: ({ci_of_oob[0]}, {ci_of_oob[1]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKTnJdiBVS_e"
      },
      "source": [
        "# <font color='blue'><b>Task 3</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXxrvZqHV1Fr"
      },
      "source": [
        "<font color='orange'><b>Flowchart for Task 3</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyjwEJ62V6a6"
      },
      "source": [
        "<b>Hint: </b> We created 30 models by using 30 samples in TASK-1. Here, we need send query point \"xq\"  to 30 models and perform the regression on the output generated by 30 models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0emSwLL7VurD"
      },
      "source": [
        "![alt text](https://i.imgur.com/Y5cNhQk.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29hjwKlWWDfo"
      },
      "source": [
        "*  <font color='blue'><b> Write code for TASK 3 </b></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_pUlSD-VYD1",
        "outputId": "96f15330-9383-4142-a164-170786a73fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Y = 19.4\n"
          ]
        }
      ],
      "source": [
        "# predict the yq-value for the given xq\n",
        "\n",
        "xq = np.array([0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60])\n",
        "y_vals = []\n",
        "\n",
        "for i in range(30):\n",
        "    datapoint = xq[list_selected_columns[i]]\n",
        "    y_vals.append(models[i].predict(datapoint.reshape(1,-1)))\n",
        "print(f'Predicted Y = {np.median(y_vals)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJHTGEZgWJjR"
      },
      "source": [
        "<br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOdUi-0xWOJ9"
      },
      "source": [
        "<font color='red'><b>Write observations for task 1, task 2, task 3 indetail</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIcax45hWKT-"
      },
      "source": [
        "### Task 1\n",
        "<br></br>\n",
        "#### Step 1 - Generating Bootstrap Samples\n",
        "* For Step 1 we were focussed on re-creating the concept of bootstrap samples from the original dataset. The size of the bootstrap samples should be same as that of the original dataset. \n",
        "- For this first sample we selected 60% of the datapoints from the original dataset ***without replacement***. Let's call this sample1. \n",
        "* Now using sample1 from the above step we again perform sampling ****with replacement**** for the remaining 40% of the points of our new dataset.\n",
        "* We stack this two samples together to form a new dataset which is same in size as our original dataset.\n",
        "* Apart form row sampling we are also performing column sampling(w/o replacement) on our available dataset( one key aspect in RF).\n",
        "* Now we repeat this above process 30 times to obtain our 30 Bootsrap samples.\n",
        "\n",
        "<br></br> \n",
        "#### Step 2: Building High variance models and Calculating MSE\n",
        "* In this step we are going to build 30 high variance base models, DT Regressors(max_depth=None) one for each of the samples generated in the above step. We are also storing the models so that we can use them further.\n",
        "* After the models are fit we are going to use it to calculate the train MSE.\n",
        "+ To calculate train MSE we pass all the points in our train data through all the models. Lets take the example of one datapoint passing through this RF model.\n",
        "    * For a single point say x', first we are passing it through all the 30 DT models we have.\n",
        "    * Where for each model different columns of x' may be used. We apply the columns selected in column sampling as required for whichever model and pass that point to the corresponding model.\n",
        "    * Each model gives us a predicted y i.e. we have a list of 30 predicted y's.\n",
        "    * Now we aggregate all these 30 predicted y's using either mean or median.\n",
        "    * This gives us the y' corresponding to x'.\n",
        "* So when we pass out dataset through the model, we get a list of predicted values of x, the list of predicted y-vals.\n",
        "* Using this list of predicted y-vals, and the original/target y-vals we are going to calculate MSE, which is the average of sum of deviations square.\n",
        "\n",
        "<br></br> \n",
        "#### Step 3: Calculating OOB Score\n",
        "* For calculating OOB Score we are going to follow the following steps:\n",
        "    * for a particular datapoint xi, check in which base model this was **not used in the training stage**. In whichever model this datapoint was not used in the training stage, use that model to predict its y-value( after applying the column sampling)\n",
        "    * Now aggregate all the y-values generated(which will be less than 30) to get the final yi.\n",
        "* We repeat this step for all the datapoints in our dataset to get a list of predicted OOB y-vals.\n",
        "* Finally we use the original y-vals and predicted OOB y-vals to obtain our OOB Score as per the formula.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRpDaEukJg5y"
      },
      "source": [
        "### Task 2 : Calculate CI of MSE and OOB Score\n",
        "<br></br>\n",
        "* We are going to repeat Task1 35 times to get a list of 35 MSE and OOB score.\n",
        "* Assuming this as a sample of MSE and OOB Score we are going to use it to calculate our 95% Confidence Interval.\n",
        "* For calculation of CI we need to calculate the mean and Standard Errow of MSE and OOB Scores.\n",
        "    * For lower limit of CI of MSE: $ mean(MSE) - 1.96 * SE(MSE)/  sq.root(n) $ <br></br>\n",
        "    * For upper limit of CI of MSE: $ mean(MSE) + 1.96 * SE(MSE)/  sq.root(n) $ <br></br>\n",
        "where n is the number of datapoints in the sample\n",
        "\n",
        "* We can repeat the same for OOB score values, which will give us the lower and the upper limit of CI OOB Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co-Mq2ktJg5y"
      },
      "source": [
        "### Task 3 : Predict yq given xq.\n",
        "\n",
        "* Given a query point we have to pass it through all the base models and get the predicted value from each model and the aggregate them using median to get our final predicted y value."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}