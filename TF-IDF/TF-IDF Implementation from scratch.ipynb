{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvLvmewlxLix"
      },
      "source": [
        "<font face='georgia'>\n",
        "    \n",
        "   <h4><strong>What does tf-idf mean?</strong></h4>\n",
        "\n",
        "   <p>    \n",
        "Tf-idf stands for <em>term frequency-inverse document frequency</em>, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
        "</p>\n",
        "    \n",
        "   <p>\n",
        "One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n",
        "</p>\n",
        "    \n",
        "   <p>\n",
        "Tf-idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.\n",
        "</p>\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XLaGFUMxLiy"
      },
      "source": [
        "<font face='georgia'>\n",
        "    <h4><strong>How to Compute:</strong></h4>\n",
        "\n",
        "Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
        "\n",
        " <ul>\n",
        "    <li>\n",
        "<strong>TF:</strong> Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: <br>\n",
        "\n",
        "$TF(t) = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}.$\n",
        "</li>\n",
        "<li>\n",
        "<strong>IDF:</strong> Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: <br>\n",
        "\n",
        "$IDF(t) = \\log_{e}\\frac{\\text{Total  number of documents}} {\\text{Number of documents with term t in it}}.$\n",
        "for numerical stabiltiy we will be changing this formula little bit\n",
        "$IDF(t) = \\log_{e}\\frac{\\text{Total  number of documents}} {\\text{Number of documents with term t in it}+1}.$\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "<br>\n",
        "<h4><strong>Example</strong></h4>\n",
        "<p>\n",
        "\n",
        "Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
        "</p>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAcTjhO8xLiz"
      },
      "source": [
        "<font face='georgia'>\n",
        "    <h4><strong>1. Building a TFIDF Vectorizer & comparing its results with Sklearn:</strong></h4>\n",
        "\n",
        "<ul>\n",
        "    <li> As a part of this mini task we will be implementing TFIDF vectorizer on a collection of text documents.</li>\n",
        "    <br>\n",
        "    <li> We will compare the results of our own implementation of TFIDF vectorizer with that of sklearns implemenation TFIDF vectorizer.</li>\n",
        "    <br>\n",
        "    <li> Sklearn does few more tweaks in the implementation of its version of TFIDF vectorizer, so to replicate the exact results we will need to add following things into our custom implementation of tfidf vectorizer:\n",
        "       <ol>\n",
        "        <li> Sklearn has its vocabulary generated from idf sorted in alphabetical order</li>\n",
        "        <li> Sklearn's formula of idf is different from the standard textbook formula. Here the constant <strong>\"1\"</strong> is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions.\n",
        "            \n",
        " $IDF(t) = 1+\\log_{e}\\frac{1\\text{ }+\\text{ Total  number of documents in collection}} {1+\\text{Number of documents with term t in it}}.$\n",
        "        </li>\n",
        "        <li> Sklearn applies L2-normalization on its output matrix.</li>\n",
        "        <li> The final output of sklearn tfidf vectorizer is a sparse matrix.</li>\n",
        "    </ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnV82tg1xLi0"
      },
      "source": [
        "### Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUsYm9wjxLi1"
      },
      "outputs": [],
      "source": [
        "## SkLearn# Collection of string documents\n",
        "\n",
        "corpus = [\n",
        "     'this is the first document',\n",
        "     'this document is the second document',\n",
        "     'and this is the third one',\n",
        "     'is this the first document',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLwmFZfKxLi4"
      },
      "source": [
        "### SkLearn Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np4dfQOkxLi4"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "skl_output = vectorizer.transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7Om8YpYxLi6",
        "outputId": "0a3bd0f5-4424-4400-944f-4482a80bd799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ]
        }
      ],
      "source": [
        "# sklearn feature names, they are sorted in alphabetic order by default.\n",
        "\n",
        "print(vectorizer.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTKplK96xLi-",
        "outputId": "53722fa2-6756-4aa0-f179-37b578bb6890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
            " 1.         1.91629073 1.        ]\n"
          ]
        }
      ],
      "source": [
        "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
        "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
        "\n",
        "print(vectorizer.idf_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CTiWHygxLjA",
        "outputId": "8d5a9cde-2c29-4afe-f7b4-1547e88dba4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 9)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
        "\n",
        "skl_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDKEpbA-xLjD",
        "outputId": "87dafd65-5313-443f-8c6e-1b05cc8c2543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 8)\t0.38408524091481483\n",
            "  (0, 6)\t0.38408524091481483\n",
            "  (0, 3)\t0.38408524091481483\n",
            "  (0, 2)\t0.5802858236844359\n",
            "  (0, 1)\t0.46979138557992045\n"
          ]
        }
      ],
      "source": [
        "# sklearn tfidf values for first line of the above corpus.\n",
        "# Here the output is a sparse matrix\n",
        "\n",
        "print(skl_output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QWo34hexLjF",
        "outputId": "cdc04e08-989f-4bdc-dd7f-f1c82a9f90be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ],
      "source": [
        "# sklearn tfidf values for first line of the above corpus.\n",
        "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
        "# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n",
        "\n",
        "print(skl_output[0].toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfIwx5LzxLjI"
      },
      "source": [
        "### Our custom implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmcWiDfF5yjQ"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "import operator\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def fit(dataset):\n",
        "    \"\"\"\n",
        "    This function returns the dictionary of all the unique words in the corpus and the idf values for each\n",
        "    of the unique words.\n",
        "    *******************************************************************************************************\n",
        "    Input: \"dataset\": A list of sentences\n",
        "    Output: returns vocab, idf_values\n",
        "    where: vocab - is a dictionary containing all the unique words as keys and the column index as values\n",
        "           idf_values - also a dictionary consisting of all the unique words and their idf values as key:value pairs\n",
        "    \"\"\"\n",
        "    \n",
        "    if isinstance(dataset, (list,)):\n",
        "        unique_words = set()\n",
        "        for row in dataset:\n",
        "            for word in row.split():\n",
        "                if len(word)<2:\n",
        "                    continue\n",
        "                unique_words.add(word)\n",
        "        unique_words = sorted(list(unique_words))\n",
        "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
        "        \n",
        "        # This is the implementation of idf\n",
        "        N = len(dataset)                      # Total no.of documents\n",
        "\n",
        "        # We need to find the idf values for each word in the vocab \n",
        "        # We need the count the number of documents with a particular word(of the vocab)\n",
        "        idf_freq = {x:0 for x in vocab.keys()}\n",
        "        for term in vocab.keys():              # for each word in the vocab\n",
        "            for row in dataset:                # for each document in the corpus\n",
        "                if term in row.split():\n",
        "                    idf_freq[term]+=1          # count if the word is present in the document\n",
        "        \n",
        "        # Calculating the idf value\n",
        "        idf_ = {x:0 for x in vocab.keys()}\n",
        "        for term in idf_.keys():\n",
        "            idf_[term] = 1+ np.log((1+N)/(1+idf_freq[term]))      # Sklearn's modified formula for caclculating idf values\n",
        "        \n",
        "        return vocab,idf_\n",
        "    else:\n",
        "        print(\"Please pass the input as a list of sentences\")\n",
        "        \n",
        "\n",
        "# implementing the tf_idf\n",
        "# for this we need to have the dataset\n",
        "def transform(dataset):\n",
        "    \"\"\"\n",
        "    This function returns the tf_idf vectors for each document in the corpus\n",
        "    ********************************************************************************\n",
        "    Input: The corpus or 'dataset' which should be a list of sentences.\n",
        "    Output: This function returns the tf_idf vector for each document in the corpus.\n",
        "    \n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    columns = []\n",
        "    values = []\n",
        "    if isinstance(dataset,(list,)):\n",
        "        vocab, idf_vals = fit(dataset)             # Fetching the unique words and the IDF values.\n",
        "\n",
        "        for idx,row in enumerate(dataset):\n",
        "            # the problem is that we need to use each column of the vocabulary \n",
        "\n",
        "            tf = dict(Counter(row.split()))         # this is counting the frequency of the term in a particular row\n",
        "\n",
        "            s = sum(tf.values())                    # this is to calculate the total number of words in a row \n",
        "\n",
        "            tf = {x:tf[x]/s for x in tf.keys()}     # Dividing the values by the total number of words in a row\n",
        "       \n",
        "\n",
        "            tf_idf = {x:0 for x in vocab.keys()}    # Creating an empty\n",
        "            for i in vocab.keys():\n",
        "                # We are using try except block to assign tf_idf values for each document in the corpus. Why?\n",
        "                # The reason being that in a document it is not necessary that all the unique words will be present,\n",
        "                # some of them will not be, which will not be counted while calculating the tf values. \n",
        "                # So if a word in not present in a document then it won't be present in the term freq dictionary, and will\n",
        "                # throw a KerError to us when we try to acces that word in the dictionary. \n",
        "                # We are usign this fact to assign the tf_idf(word) = 0(when word in not present in a document)\n",
        "                try:\n",
        "                    tf_idf[i] = tf[i]*idf_vals[i]   # calculating the tf_idf values\n",
        "                except KeyError:\n",
        "                    tf_idf[i] = 0\n",
        "\n",
        "# For a particular row we are going to store the tf_idf values for words that are present in that particular row/document only\n",
        "# So it is logical to iterate over the words that are present in the term frequency dictionary(since it is specific to a row)\n",
        "# discard all the other words present in the vocabulary.\n",
        "            \n",
        "            for word in tf.keys():                         # for each word present in a particular document\n",
        "                if len(word)<2:\n",
        "                    continue\n",
        "                # replace a with vocab\n",
        "                col_idx = vocab.get(word, -1)              # get the column index for the word\n",
        "                if col_idx!=-1:\n",
        "                    # Store the row,column index and the tf_idf values\n",
        "                    columns.append(col_idx)                \n",
        "                    rows.append(idx)\n",
        "                    values.append(tf_idf[word])\n",
        "        \n",
        "        values = csr_matrix((values, (rows,columns)), shape=(len(corpus), len(vocab)))\n",
        "        values = normalize(values)                         # Normalising the values in the sparse matrix\n",
        "        return values\n",
        "    else:\n",
        "        print('Pass the input as a list of sentences')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj4uw9l45yjS"
      },
      "outputs": [],
      "source": [
        "vocab, idf_vals= fit(corpus)\n",
        "out = transform(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDhagafy5yjS"
      },
      "source": [
        "# Compairing the results with Sklearn's implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuw8aa5a5yjT",
        "outputId": "11670857-4f8e-4807-847e-cf206947840e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom Implementation:\n",
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "Sklearn's Implementation:\n",
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ]
        }
      ],
      "source": [
        "# The unique words in a corpus\n",
        "print(\"Custom Implementation:\")\n",
        "print(list(vocab.keys()))\n",
        "print(\"Sklearn's Implementation:\")\n",
        "print(vectorizer.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWN-r2045yjU",
        "outputId": "063e0952-5790-48bf-daf8-5c5c03a81a5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom Implementation:\n",
            "[1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n",
            "Sklearn's Implementation:\n",
            "[1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n"
          ]
        }
      ],
      "source": [
        "# Idf values\n",
        "print(\"Custom Implementation:\")\n",
        "print(list(idf_vals.values()))\n",
        "print(\"Sklearn's Implementation:\")\n",
        "print(list(vectorizer.idf_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9DY324-5yjV",
        "outputId": "50e18713-e225-4d8f-ad86-532b1e7e1158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom Implementation:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "Sklearn's Implementation:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ],
      "source": [
        "# TF-IDF values for the first document\n",
        "print(\"Custom Implementation:\")\n",
        "print(out[0].toarray())\n",
        "print(\"Sklearn's Implementation:\")\n",
        "print(skl_output[0].toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51j_OtqAxLjL"
      },
      "source": [
        "<font face='georgia'>\n",
        "    <h4><strong>2. Implement max features functionality:</strong></h4>\n",
        "\n",
        "<ul>\n",
        "    <li> As a part of this task we will modify our fit and transform functions so that our vocab will contain only 50 terms with top idf scores.</li>\n",
        "    <br>\n",
        "    <li>This task is similar to our previous task, just that here our vocabulary is limited to only top 50 features names based on their idf values. Basically our output will have exactly 50 columns and the number of rows will depend on the number of documents we have in our corpus.</li>\n",
        "    <br>\n",
        "    <li>Here we have a pickle file, with file name <strong>cleaned_strings</strong>. We have to load the corpus from this file and use it as input to our tfidf vectorizer.</li>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHxPLlwNxLjL",
        "outputId": "9abd8e08-0e24-4975-9a13-4d3636d60323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents in corpus =  746\n"
          ]
        }
      ],
      "source": [
        "# Below is the code to load the cleaned_strings pickle file provided\n",
        "# Here corpus is of list type\n",
        "\n",
        "import pickle\n",
        "with open('cleaned_strings', 'rb') as f:\n",
        "    corpus = pickle.load(f)\n",
        "    \n",
        "# printing the length of the corpus loaded\n",
        "print(\"Number of documents in corpus = \",len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZULfoOIdxLjQ"
      },
      "outputs": [],
      "source": [
        "# Write your code here.\n",
        "# Try not to hardcode any values.\n",
        "# Make sure its well documented and readble with appropriate comments.\n",
        "\n",
        "\n",
        "def fit(dataset):\n",
        "    \"\"\"\n",
        "    This function only returns the vocabulary/dictionary of all the unique words in the corpus\n",
        "    *******************************************************************************************************\n",
        "    Input: \"dataset\": A list of sentences\n",
        "    Output: returns vocab, idf_values\n",
        "    where: vocab - is a dictionary containing all the unique words as keys and the column index as values\n",
        "           idf_values - also a dictionary consisting of all the unique words and their idf values as key:value pairs\n",
        "    \"\"\"\n",
        "    \n",
        "    if isinstance(dataset, (list,)):\n",
        "        # These lines of code are just to find the set of unique words and give them a column number in a sorted order. \n",
        "        unique_words = set()\n",
        "        for row in dataset:\n",
        "            for word in row.split():\n",
        "                if len(word)<2:\n",
        "                    continue\n",
        "                unique_words.add(word)\n",
        "        unique_words = sorted(list(unique_words))\n",
        "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
        "        \n",
        "        # The implementation of idf starts here\n",
        "        N = len(dataset)                      # Total no.of documents\n",
        "\n",
        "        # We need to find the idf values for each word in the vocab \n",
        "        # We need the count the number of documents with a particular word(of the vocab)\n",
        "        idf_freq = {x:0 for x in vocab.keys()}\n",
        "        for term in vocab.keys():\n",
        "            for row in dataset:\n",
        "                if term in row.split():\n",
        "                    idf_freq[term]+=1\n",
        "        \n",
        "        # Calculating the idf value\n",
        "        idf_ = {x:0 for x in vocab.keys()}\n",
        "        for term in idf_.keys():                                      # for each unique word in the corpus \n",
        "            idf_[term] = 1+ np.log((1+N)/(1+idf_freq[term]))          # calculating the idf values\n",
        "        \n",
        "        n = 50    # the reduced size of the vocabulary\n",
        "        f = [(idf_[x],x) for x in idf_.keys()]                  # storing the idf,words as a list of tuples\n",
        "        f = sorted(f,reverse=True)[0:n]                # sorting by the idf values and taking only the top 50 idf value words\n",
        "        new_idf = {x:y for y,x in f}\n",
        "        new_vocab = {i:j for j,i in enumerate(new_idf.keys())}   # Storing the words and the idf values in a dictionary\n",
        "        \n",
        "        return new_vocab,new_idf\n",
        "    else:\n",
        "        print(\"Please pass the input as a list of sentences\")\n",
        "        \n",
        "\n",
        "# implementing the tf_idf\n",
        "# for this we need to have the dataset\n",
        "def transform(dataset):\n",
        "    \"\"\"\n",
        "    This function returns the tf_idf vectors for each document in the corpus\n",
        "    ********************************************************************************\n",
        "    Input: The corpus or 'dataset' which should be a list of sentences.\n",
        "    Output: This function returns the tf_idf vector for each document in the corpus.\n",
        "    \n",
        "    \"\"\"\n",
        "    # Implementing the tf_idf freq\n",
        "    \n",
        "    rows = []\n",
        "    columns = []\n",
        "    values = []\n",
        "    if isinstance(dataset,(list,)):\n",
        "        \n",
        "        vocab, idf_vals = fit(dataset)                      # get the dictionary of unique words and their column index\n",
        "                                                            # get the idf values for each unique word in the corpus\n",
        "        \n",
        "        for idx,row in enumerate(dataset):\n",
        "\n",
        "            tf = dict(Counter(row.split()))         # this is counting the frequency of the words in a particular document\n",
        "\n",
        "            s = sum(tf.values())                    # this is to calculate the total number of words in a document\n",
        "\n",
        "            tf = {x:tf[x]/s for x in tf.keys()}     # Normalising the sum \n",
        "       \n",
        "\n",
        "            tf_idf = {x:0 for x in vocab.keys()}    # Creating an empty dictionary to store the tf_idf values\n",
        "            for i in vocab.keys():\n",
        "                # We are using try except block to assign tf_idf values for each document in the corpus. Why?\n",
        "                # The reason being that in a document it is not necessary that all the unique words will be present,\n",
        "                # some of them will not be, which will not be counted while calculating the tf values. \n",
        "                # So if a word in not present in a document then it won't be present in the term freq dictionary, and will\n",
        "                # throw a KerError to us when we try to acces that word in the dictionary. \n",
        "                # We are usign this fact to assign the tf_idf(word) = 0(when word in not present in a document)\n",
        "                try:\n",
        "                    tf_idf[i] = tf[i]*idf_vals[i]   # calculating the tf_idf values\n",
        "                except KeyError:\n",
        "                    tf_idf[i] = 0\n",
        "\n",
        "# For a particular row we are going to store the tf_idf values for words that are present in that particular row/document only\n",
        "# So it is logical to iterate over the words that are present in the term frequency dictionary(since it is specific to a row)\n",
        "# discard all the other words present in the vocabulary cause their tf_idf values will be zero anyway.\n",
        "            \n",
        "            for word in tf.keys():                         # for each word present in a particular document\n",
        "                if len(word)<2:\n",
        "                    continue\n",
        "                # replace a with vocab\n",
        "                col_idx = vocab.get(word, -1)              # get the column index for the word\n",
        "                if col_idx!=-1:\n",
        "                    # Store the row,column index and the tf_idf values\n",
        "                    columns.append(col_idx)                \n",
        "                    rows.append(idx)\n",
        "                    values.append(tf_idf[word])\n",
        "        \n",
        "        values = csr_matrix((values, (rows,columns)), shape=(len(corpus), len(vocab)))\n",
        "        values = normalize(values)                         # Normalising the values in the sparse matrix\n",
        "        return values\n",
        "    else:\n",
        "        print('Pass the input as a list of sentences')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_DJnnR3xLjR"
      },
      "outputs": [],
      "source": [
        "vocab, idf = fit(corpus)\n",
        "tf_idf = transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmwIM_ln5yjc",
        "outputId": "c7ac1d05-b8b3-40e9-cae9-f81403aafd7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The words with top idf values in descending order:\n",
            "['zombiez', 'zillion', 'yun', 'youtube', 'youthful', 'younger', 'yelps', 'yawn', 'yardley', 'wrote', 'writers', 'wrap', 'wow', 'woven', 'wouldnt', 'worthy', 'worthwhile', 'worthless', 'worry', 'worked', 'woo', 'wont', 'wong', 'wondered', 'woa', 'witticisms', 'within', 'wise', 'win', 'wily', 'willie', 'william', 'wild', 'wih', 'wife', 'widmark', 'wide', 'whoever', 'whites', 'whine', 'whenever', 'went', 'welsh', 'weight', 'wedding', 'website', 'weaving', 'weariness', 'weaker', 'wb']\n"
          ]
        }
      ],
      "source": [
        "print(\"The words with top idf values in descending order:\")\n",
        "print(list(vocab.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJjoAl725yjd",
        "outputId": "afc0a7aa-61f5-4b72-b31e-56e1fbcffea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The idf values in descending order:\n",
            "[6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872]\n"
          ]
        }
      ],
      "source": [
        "print(\"The idf values in descending order:\")\n",
        "print(list(idf.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVc9XW3M5yjd",
        "outputId": "798632d4-df71-445c-bab1-966ca194ac6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 11)\t0.37796447300922725\n",
            "  (0, 24)\t0.37796447300922725\n",
            "  (0, 25)\t0.37796447300922725\n",
            "  (0, 27)\t0.37796447300922725\n",
            "  (0, 33)\t0.37796447300922725\n",
            "  (0, 45)\t0.37796447300922725\n",
            "  (0, 47)\t0.37796447300922725\n"
          ]
        }
      ],
      "source": [
        "# Output for a single document\n",
        "print(tf_idf[135])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "w3Ib4iSB5yje",
        "outputId": "6a5411e8-1d52-432f-b3a2-1461a9fc6ae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.37796447\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.37796447 0.37796447 0.         0.37796447 0.         0.\n",
            "  0.         0.         0.         0.37796447 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.37796447 0.         0.37796447\n",
            "  0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Converting the output to a dense representation\n",
        "print(tf_idf[135].toarray())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}